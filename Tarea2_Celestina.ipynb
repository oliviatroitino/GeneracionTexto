{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_ahxalS_tMt"
      },
      "source": [
        "## Setup paquetes y Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "CyLNCN5Q8jNE"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-dPaX2C_xSW"
      },
      "source": [
        "## Descarga y preprocesamiento de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8H9BVBBiB4ZK"
      },
      "source": [
        "Hay cambios del notebook de partida (de encoding `utf-8` a `latin-1`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duysNv6Y_ztb",
        "outputId": "efc970c2-1cc3-4fd9-fed2-ed8824eaaced"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Longitud del texto:        376026 carácteres\n",
            "El texto está compuesto de estos 90 carácteres:\n",
            "['\\n', ' ', '!', '\"', '#', '&', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '>', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'X', 'Y', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'x', 'y', 'z', '{', '}', '~', '¡', '¿', 'Ü', 'á', 'ç', 'é', 'í', 'ñ', 'ó', 'ú']\n"
          ]
        }
      ],
      "source": [
        "texto = open(\"3_textos_literatura_española/La Celestina.txt\", 'rb').read().decode(encoding='latin-1')\n",
        "print('Longitud del texto:        {} carácteres'.format(len(texto)))\n",
        "\n",
        "vocab = sorted(set(texto))\n",
        "\n",
        "print ('El texto está compuesto de estos {} carácteres:'.format(len(vocab)))\n",
        "print (vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sZ3XIMVCFdh"
      },
      "source": [
        "### Procesamiento de los textos\n",
        "#### Mapeo de caracteres"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Of1auErLCHFP",
        "outputId": "15a85016-4fee-4dda-d677-4847676221ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  '\\n':   0,\n",
            "  ' ' :   1,\n",
            "  '!' :   2,\n",
            "  '\"' :   3,\n",
            "  '#' :   4,\n",
            "  '&' :   5,\n",
            "  '*' :   6,\n",
            "  ',' :   7,\n",
            "  '-' :   8,\n",
            "  '.' :   9,\n",
            "  '/' :  10,\n",
            "  '0' :  11,\n",
            "  '1' :  12,\n",
            "  '2' :  13,\n",
            "  '3' :  14,\n",
            "  '4' :  15,\n",
            "  '5' :  16,\n",
            "  '6' :  17,\n",
            "  '7' :  18,\n",
            "  '8' :  19,\n",
            "  '9' :  20,\n",
            "  ':' :  21,\n",
            "  ';' :  22,\n",
            "  '<' :  23,\n",
            "  '>' :  24,\n",
            "  '?' :  25,\n",
            "  'A' :  26,\n",
            "  'B' :  27,\n",
            "  'C' :  28,\n",
            "  'D' :  29,\n",
            "  'E' :  30,\n",
            "  'F' :  31,\n",
            "  'G' :  32,\n",
            "  'H' :  33,\n",
            "  'I' :  34,\n",
            "  'J' :  35,\n",
            "  'L' :  36,\n",
            "  'M' :  37,\n",
            "  'N' :  38,\n",
            "  'O' :  39,\n",
            "  'P' :  40,\n",
            "  'Q' :  41,\n",
            "  'R' :  42,\n",
            "  'S' :  43,\n",
            "  'T' :  44,\n",
            "  'U' :  45,\n",
            "  'V' :  46,\n",
            "  'X' :  47,\n",
            "  'Y' :  48,\n",
            "  '^' :  49,\n",
            "  '_' :  50,\n",
            "  '`' :  51,\n",
            "  'a' :  52,\n",
            "  'b' :  53,\n",
            "  'c' :  54,\n",
            "  'd' :  55,\n",
            "  'e' :  56,\n",
            "  'f' :  57,\n",
            "  'g' :  58,\n",
            "  'h' :  59,\n",
            "  'i' :  60,\n",
            "  'j' :  61,\n",
            "  'k' :  62,\n",
            "  'l' :  63,\n",
            "  'm' :  64,\n",
            "  'n' :  65,\n",
            "  'o' :  66,\n",
            "  'p' :  67,\n",
            "  'q' :  68,\n",
            "  'r' :  69,\n",
            "  's' :  70,\n",
            "  't' :  71,\n",
            "  'u' :  72,\n",
            "  'v' :  73,\n",
            "  'x' :  74,\n",
            "  'y' :  75,\n",
            "  'z' :  76,\n",
            "  '{' :  77,\n",
            "  '}' :  78,\n",
            "  '~' :  79,\n",
            "  '¡' :  80,\n",
            "  '¿' :  81,\n",
            "  'Ü' :  82,\n",
            "  'á' :  83,\n",
            "  'ç' :  84,\n",
            "  'é' :  85,\n",
            "  'í' :  86,\n",
            "  'ñ' :  87,\n",
            "  'ó' :  88,\n",
            "  'ú' :  89,\n"
          ]
        }
      ],
      "source": [
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "for char,_ in zip(char2idx, range(len(vocab))):\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlXmg61nDxFT"
      },
      "source": [
        "Pasamos cada texto a un array de enteros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IHo-ePmDwih",
        "outputId": "c1e658c9-f6bd-41b5-9c97-535d76da2982"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "texto : 'La Celestina {0}\\n---------------------------------'\n",
            "'La Celestina {0}\\n---------------------------------'\n"
          ]
        }
      ],
      "source": [
        "text_as_int = np.array([char2idx[c] for c in texto])\n",
        "\n",
        "print ('texto : {}'.format(repr(texto[:50])))\n",
        "print ('{}'.format(repr(texto[:50])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMTjafW0Eaz2"
      },
      "source": [
        "### Preparación de los datos para entrenar la RNN\n",
        "\n",
        "Para entrenar el modelo creamos un conjunto de datos con el contenido de text_as_init. Para ello utilizamos la función tf.data.Dataset.from_tensor_slices.\n",
        "A este conjunto de datos lo dividiremos en secuencias de seq_length+1 al aplicar el método batch()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "9gOSC4lCFY8y"
      },
      "outputs": [],
      "source": [
        "# Creamos una función `split_input_target` que devolverá el conjunto de datos\n",
        "# de entrenamiento (los datos de entrada como los datos de salida)\n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "#Agrupamos los dataset en batches de 64 .\n",
        "# Así tendriamos los datos de entrenamiento con batches compuestos de 64 parejas\n",
        "# de secuencias de 100 integers de 64 bits\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wv39xu08EfGl",
        "outputId": "5abe4904-bf33-480a-cc21-9a64de6b43ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'La Celestina {0}\\n------------------------------------------------------------------------\\n-----------'\n",
            "'-------------------------------------------------------------\\nTRAGICOMEDIA DE CALISTO Y MELIBEA: Text'\n",
            "'o\\n------------------------------------------------------------------------\\n\\n*SIGUESE*\\n\\nla comedia /o '\n",
            "'tragicomedia / de calisto y melibea, compuesta en\\nreprehension de los locos enamorados, que, vencidos'\n",
            "' en su desordenado\\napetito, a sus amigas llaman y dizen ser su Dios. Assi mesmo fecha en\\nauiso de los'\n",
            "' engaños de las alcahuetas y malos y lisonjeros siruientes.\\n\\n----------------------------------------'\n",
            "'--------------------------------\\n*ARGUMENTO GENERAL*\\n\\n*\\n---------------------------------------------'\n",
            "'---------------------------\\nARGUMENTO de toda la obra\\n\\n*Calisto fue de noble linaje, de claro ingenio'\n",
            "', de gentil disposición,\\nde linda criança, dotado de muchas gracias, de estado mediano. Fue preso\\nen '\n",
            "'el amor de Melibea, {14\"} muger moça, muy generosa, de alta y\\nsereníssima sangre, sublimada en próspe'\n",
            "\n",
            "\n",
            "\n",
            "Input data:  'La Celestina {0}\\n------------------------------------------------------------------------\\n----------'\n",
            "Target data: 'a Celestina {0}\\n------------------------------------------------------------------------\\n-----------'\n",
            "<_MapDataset element_spec=(TensorSpec(shape=(100,), dtype=tf.int64, name=None), TensorSpec(shape=(100,), dtype=tf.int64, name=None))>\n",
            "<_BatchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>\n"
          ]
        }
      ],
      "source": [
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "seq_length = 100\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for item in sequences.take(10):\n",
        "  print(repr(''.join(idx2char[item.numpy()])))\n",
        "\n",
        "#Aplicamos split_input_target a todas las secuencias utilizando el método map()\n",
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "#Los dataset contienen un conjunto de parejas (100 caracteres del texto original, la correspondiente salida ). Vamos a mostrar la primera pareja.\n",
        "for input_example, target_example in  dataset.take(1):\n",
        "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))\n",
        "\n",
        "  print(dataset)\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "print (dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pzc_abq3Gtra"
      },
      "source": [
        "### Construcción del modelo RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "s9Uxb2ALG0kv"
      },
      "outputs": [],
      "source": [
        "#Crearemos una función que cree un modelo RNN con tres capas\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, BatchNormalization\n",
        "\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = Sequential()\n",
        "  #Añadimos la capa de tipo word embedding\n",
        "  model.add(Embedding(input_dim=vocab_size,\n",
        "                      output_dim=embedding_dim,\n",
        "                      #batch_input_shape=[batch_size, None] Deprecated\n",
        "                      ))\n",
        "  #Añadimos la capa de tipo LSTM\n",
        "  model.add(LSTM(rnn_units,\n",
        "                 return_sequences=True,\n",
        "                 stateful=True,\n",
        "                 recurrent_initializer='glorot_uniform'))\n",
        "  model.add(Dense(512, activation=\"relu\"))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.4))\n",
        "\n",
        "\n",
        "  #Añadimos la capa de tipo Dense\n",
        "  model.add(Dense(vocab_size))\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "CXSrXhXHG-Vh"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 256\n",
        "rnn_units = 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "YkxLgKFMHLoB",
        "outputId": "db74e307-2006-44f7-cf33-1405503bbebf"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_8\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential_8\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_2           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_8 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_8 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_16 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_2           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_17 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model = build_model(\n",
        "  vocab_size = len(vocab),\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tH80YLm9IC5O",
        "outputId": "2b325b7a-55ff-4d41-f42e-6152e52a4ef6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input: (64, 100) # (batch_size, sequence_length)\n",
            "Target: (64, 100) # (batch_size, sequence_length)\n"
          ]
        }
      ],
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  print(\"Input:\", input_example_batch.shape, \"# (batch_size, sequence_length)\")\n",
        "  print(\"Target:\", target_example_batch.shape, \"# (batch_size, sequence_length)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VfJiDVjVIFMx",
        "outputId": "a39a8a7b-92e1-4a91-edba-8c97a5c515cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction :  (64, 100, 90) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ],
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(\"Prediction : \", example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "j8ODv1oxIGxL"
      },
      "outputs": [],
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices_characters = tf.squeeze(sampled_indices,axis=-1).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5s5oc_-5Il7k",
        "outputId": "8632e78a-22ca-4369-feb9-c1db9a06cb78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[45 25 89 40 17 70 37 15 54 13 50 52 36  2 68 39  1 38  9 21  0 47 79  2\n",
            " 49 67 62 51 29 14 83  8 83 28  5 31 58 80 71 41 23  6 28 87  5 68 18  1\n",
            "  6 15 45 83  3 49 18 24 16 15 21 82 44 76 36 61 72 62 79 13 29 22 45 44\n",
            " 50 68 82 66 27 83  0 14 36 87 20 81 85 19 73 69  4 25 43 75 89 81 39 15\n",
            " 75 74 44 26]\n"
          ]
        }
      ],
      "source": [
        "print(sampled_indices_characters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJENmFNXL0Io"
      },
      "source": [
        "### Entrenamiento del modelo RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "uKqSJOP_Cpso"
      },
      "outputs": [],
      "source": [
        "#Creamos la función de perdida, usaremos el categorical pues estamos considerando datos categóricos\n",
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "YPNOYQq9Cpu-"
      },
      "outputs": [],
      "source": [
        "#Compilamos el modelo\n",
        "model.compile(optimizer='adam', loss=loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "qqxhp8m-Cpw1"
      },
      "outputs": [],
      "source": [
        "#configuramos los checkpoints\n",
        "\n",
        "checkpoint_dir = './training_checkpoints_Celestina'\n",
        "\n",
        "# nombre fichero\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}.weights.h5\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implementación EarlyStopping\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stopping_callback = EarlyStopping(\n",
        "    monitor='loss',\n",
        "    patience=10,\n",
        "    min_delta=0.01,\n",
        "    restore_best_weights=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "SROaEjLNDCuF",
        "outputId": "7ca79371-a2d0-4f52-de56-d96f2755f556"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 573ms/step - loss: 3.1028\n",
            "Epoch 2/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 555ms/step - loss: 2.1175\n",
            "Epoch 3/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 564ms/step - loss: 1.9808\n",
            "Epoch 4/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 570ms/step - loss: 1.8922\n",
            "Epoch 5/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 570ms/step - loss: 1.8274\n",
            "Epoch 6/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 569ms/step - loss: 1.7634\n",
            "Epoch 7/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 568ms/step - loss: 1.6998\n",
            "Epoch 8/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 570ms/step - loss: 1.6493\n",
            "Epoch 9/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 566ms/step - loss: 1.5884\n",
            "Epoch 10/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 575ms/step - loss: 1.5467\n",
            "Epoch 11/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 571ms/step - loss: 1.5014\n",
            "Epoch 12/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 568ms/step - loss: 1.4617\n",
            "Epoch 13/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 569ms/step - loss: 1.4178\n",
            "Epoch 14/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 588ms/step - loss: 1.3762\n",
            "Epoch 15/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 596ms/step - loss: 1.3348\n",
            "Epoch 16/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 583ms/step - loss: 1.2882\n",
            "Epoch 17/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 575ms/step - loss: 1.2455\n",
            "Epoch 18/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 579ms/step - loss: 1.1917\n",
            "Epoch 19/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 582ms/step - loss: 1.1364\n",
            "Epoch 20/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 574ms/step - loss: 1.0858\n",
            "Epoch 21/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 593ms/step - loss: 1.0244\n",
            "Epoch 22/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 585ms/step - loss: 0.9635\n",
            "Epoch 23/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 607ms/step - loss: 0.9046\n",
            "Epoch 24/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 598ms/step - loss: 0.8472\n",
            "Epoch 25/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 591ms/step - loss: 0.7900\n",
            "Epoch 26/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 586ms/step - loss: 0.7410\n",
            "Epoch 27/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 574ms/step - loss: 0.6884\n",
            "Epoch 28/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 585ms/step - loss: 0.6413\n",
            "Epoch 29/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 587ms/step - loss: 0.5992\n",
            "Epoch 30/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 605ms/step - loss: 0.5603\n",
            "Epoch 31/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 595ms/step - loss: 0.5238\n",
            "Epoch 32/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 583ms/step - loss: 0.4967\n",
            "Epoch 33/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 588ms/step - loss: 0.4694\n",
            "Epoch 34/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 584ms/step - loss: 0.4420\n",
            "Epoch 35/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 588ms/step - loss: 0.4244\n",
            "Epoch 36/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 588ms/step - loss: 0.4085\n",
            "Epoch 37/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 619ms/step - loss: 0.3939\n",
            "Epoch 38/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 619ms/step - loss: 0.3778\n",
            "Epoch 39/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 627ms/step - loss: 0.3627\n",
            "Epoch 40/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 623ms/step - loss: 0.3506\n",
            "Epoch 41/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 616ms/step - loss: 0.3391\n",
            "Epoch 42/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 623ms/step - loss: 0.3305\n",
            "Epoch 43/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 632ms/step - loss: 0.3215\n",
            "Epoch 44/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 633ms/step - loss: 0.3120\n",
            "Epoch 45/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 624ms/step - loss: 0.3129\n",
            "Epoch 46/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 620ms/step - loss: 0.3046\n",
            "Epoch 47/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 617ms/step - loss: 0.2999\n",
            "Epoch 48/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 619ms/step - loss: 0.2918\n",
            "Epoch 49/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 620ms/step - loss: 0.2878\n",
            "Epoch 50/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 621ms/step - loss: 0.2819\n",
            "Epoch 51/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 602ms/step - loss: 0.2774\n",
            "Epoch 52/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 627ms/step - loss: 0.2741\n",
            "Epoch 53/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 618ms/step - loss: 0.2718\n",
            "Epoch 54/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 622ms/step - loss: 0.2700\n",
            "Epoch 55/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 613ms/step - loss: 0.2649\n",
            "Epoch 56/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 610ms/step - loss: 0.2590\n",
            "Epoch 57/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 625ms/step - loss: 0.2561\n",
            "Epoch 58/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 627ms/step - loss: 0.2551\n",
            "Epoch 59/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 622ms/step - loss: 0.2506\n",
            "Epoch 60/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 610ms/step - loss: 0.2506\n",
            "Epoch 61/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 603ms/step - loss: 0.2493\n",
            "Epoch 62/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 605ms/step - loss: 0.2447\n",
            "Epoch 63/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 612ms/step - loss: 0.2465\n",
            "Epoch 64/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 614ms/step - loss: 0.2430\n",
            "Epoch 65/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 612ms/step - loss: 0.2380\n",
            "Epoch 66/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 615ms/step - loss: 0.2386\n",
            "Epoch 67/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 634ms/step - loss: 0.2349\n",
            "Epoch 68/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 627ms/step - loss: 0.2336\n",
            "Epoch 69/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 628ms/step - loss: 0.2297\n",
            "Epoch 70/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 629ms/step - loss: 0.2260\n",
            "Epoch 71/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 635ms/step - loss: 0.2245\n",
            "Epoch 72/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 626ms/step - loss: 0.2255\n",
            "Epoch 73/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 619ms/step - loss: 0.2272\n",
            "Epoch 74/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 625ms/step - loss: 0.2277\n",
            "Epoch 75/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 617ms/step - loss: 0.2239\n",
            "Epoch 76/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 607ms/step - loss: 0.2229\n",
            "Epoch 77/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 616ms/step - loss: 0.2181\n",
            "Epoch 78/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 580ms/step - loss: 0.2190\n",
            "Epoch 79/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 573ms/step - loss: 0.2154\n",
            "Epoch 80/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 571ms/step - loss: 0.2176\n",
            "Epoch 81/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 563ms/step - loss: 0.2112\n",
            "Epoch 82/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 562ms/step - loss: 0.2108\n",
            "Epoch 83/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 564ms/step - loss: 0.2127\n",
            "Epoch 84/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 572ms/step - loss: 0.2122\n",
            "Epoch 85/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 565ms/step - loss: 0.2088\n",
            "Epoch 86/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 556ms/step - loss: 0.2126\n",
            "Epoch 87/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 562ms/step - loss: 0.2112\n",
            "Epoch 88/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 564ms/step - loss: 0.2049\n",
            "Epoch 89/300\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 552ms/step - loss: 0.2089\n"
          ]
        }
      ],
      "source": [
        "#Entrenamos el modelo\n",
        "EPOCHS=300\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback, early_stopping_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "UwLoIf-eJNDH"
      },
      "outputs": [],
      "source": [
        "model.save(\"model_celestina_100_2025.keras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "Mk5PTDmwAqsJ"
      },
      "outputs": [],
      "source": [
        "from keras.models import load_model\n",
        "from keras import losses # Import the losses module\n",
        "\n",
        "# Assuming your original loss function was, for example, binary_crossentropy\n",
        "loaded_model = load_model(\"model_celestina_100_2025.keras\",\n",
        "                          custom_objects={'loss': losses.sparse_categorical_crossentropy})\n",
        "# or if it was a custom loss function\n",
        "# loaded_model = load_model(\"model_paquita_100_2024.keras\", custom_objects={'loss': my_custom_loss_function})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = build_model(len(vocab), embedding_dim, rnn_units, batch_size=1)\n",
        "input_shape = (1, 100)  # Replace 100 with your actual sequence length\n",
        "model.build(input_shape=input_shape) # Or model.build(tf.TensorShape([1, None]))\n",
        "\n",
        "model.load_weights(\"model_celestina_100_2025.keras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Creamos una función generar_texto que generará texto a partir de una palabra de partida\n",
        "def generate_text(model, start_string):\n",
        "\n",
        "  num_generate = 1000\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "  text_generated = []\n",
        "\n",
        "\n",
        "  # temperature = 0.7 # cuanto mas alto el numero, mas directa la salida de los logits\n",
        "  temperature = 0.25 # con 0.3 va mejor segun testeo con la otra clase, con 0.1 se vuelve repetitivo\n",
        "\n",
        "#  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "¿Quién eres tú, que así osas turbar mi sosiego en tan alta hora? assí\n",
            "que con la les por el tiempo que perdí de no\n",
            "gozarlo, de no conoscerlo, después que a mí me sé conoscer. No quiero\n",
            "amar mi cara ? ¿ Qué hazes ? ¿\n",
            "Porqué soy loca ? ¿ Porqué tengo fe con este couarde ? ¿ Porqué creo sus\n",
            "mentadas ? ¡ O salud de mí ! Que se llamaua el mío, que en lor este fuego de Dios, no te destroces ni maltrates como sueles. ¿\n",
            "Qué prouecho te trae dañar mis vestiduras ?\n",
            "16. CAL. __ El te me recibirá algo de tan\n",
            "alta en su pequeño y fazer las leyes de las cosas fue dizen:\n",
            "33. SEMP. __ ¿ De qué te ríes ? ¡ De mal cancre sea comida essa boca\n",
            "desgraciada, enojosa !\n",
            "36. CEL. __ ¡ Hásteme el buen seso, viendo\n",
            "la pérdida al ojo, viendo que los atauíos hazen la muger hermosa, avnque\n",
            "no lo sepa: ¿ Hauíale yo de comenido ? ¿ A quién daré\n",
            "parte de mi gloria ? Bien me dezía la vieja que de ninguna prosperidad\n",
            "es bueno a mis passados y a mí, érades compañeros; mas, quando\n",
            "el vil está rico, no tiene pariente ni amigos ? Pues, loado Dios, bienes tienes. ¿ Y no sabes que has\n",
            "men\n"
          ]
        }
      ],
      "source": [
        "print(generate_text(model, start_string=u\"¿Quién eres tú, que así osas turbar mi sosiego en tan alta hora?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Intentos con notebook base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Intento 1.1: 100 epocas y 0.3 temperatura\n",
        "```\n",
        "¿Quién eres tú, que así osas turbar mi sosiego en tan alta hora?\n",
        "15. MELIB. __ Por Dios, perdido me tiempo no la primera lisión, que dio sobre\n",
        "sano. Pues si tú quieres ser sana y que te descubra la punta de la cara. MELIB. __ Assí sea y vaya Dios contigo.\n",
        "66. PLEB. __ Señora muger, ¿ Duermes ?\n",
        "67. ALI. __ Señor, no pienses que traygo de aquella propia escriptura\n",
        "{361\"} Cota o Mena con su gran saber.\n",
        "{362\"} Jamás yo no vide en lengua romana,\n",
        "{362\"} No os lance Cupido sus tiros dorados.\n",
        "ACTO I\n",
        "```\n",
        "### Observaciones\n",
        "Las palabras son correctas, aunque algunas parezcan mal escritas (como \"muger\" o \"traygo\") son simplemente palabras escritas de la forma antigua. El estilo es coherente con el estilo de la Celestina. Los números (los cuales son ordenados en el texto original) saltan primero de 15 a 66, pero después de 66 a 67, lo cual muestra que el modelo podría estar mejor entrenado pero aún así encuentra coherencia.\n",
        "\n",
        "## Intento 1.2: 100 epocas y 0.5 temperatura\n",
        "```\n",
        "¿Quién eres tú, que así osas turbar mi sosiego en tan alta hora? Andéles, por Dios, en tal tarridar ? Tente cierto se sacaría a haglar en el cuerpo, de asco de oyrte llamar aquella\n",
        "gentil. ¡ Mirad quién gentil ! ! Jesú, Jesú ! ¡ Y qué bozos !\n",
        "11. MELIB. __ Está noche.\n",
        "37. MELIB. __ Gloriosa me será buena maestra destas\n",
        "labores. Pues agora verás quánto por mi causa vales, quánto con ello te creo. Que todas aquellas cosas, cuya possessión no es agradable, más\n",
        "vale poseellas, que esperallas. Aquí lugar de fauorecer y tu suaue vanar\n",
        "por su buena habla de comer y acompañáuala;\n",
        "tornado y tú filosofando.\n",
        "No te espero más. Saquen vn cauallo. Límpienle mucho. Aprieten bien la\n",
        "cincha perdido con el desastrado\n",
        "Píramo y de la desdichada Tisbe !\n",
        "17. SEMP. __ ¿ Qué cosa es ?\n",
        "18. CAL. __ No se ces me puede ser. No aprenden los cursos naturales a rodeando su visto, si a ti te puedo parar dispuesto de aquélla\n",
        "a quien vosotros seruís y yo adoro y, por más que trabajo noches y días,\n",
        "no me vale nombre de libre, quando cautiuaste tu voluntad.\n",
        "24. CAL. __ ¡ Palos querrá\n",
        "```\n",
        "### Observaciones\n",
        "Con la temperatura elevada empieza a inventarse palabras, como \"tarridar\", \"haglar\" o \"vn cauallo\", las cuales no se encuentran en el texto original. Los números también saltan, con dos números (en este caso 17/18) coincidiendo igual que en el intento anterior. En general se nota que el texto varía más en coherencia.\n",
        "\n",
        "## Intento 1.3: 100 epocas y 0.2 temperatura\n",
        "```\n",
        "¿Quién eres tú, que así osas turbar mi sosiego en tan alta hora?\n",
        "15. MELIB. __ ¡ O por Dios, que me matas ! ¿ Y no me marauillo que le meten de matar a tus secretos mouimientos, para saber la secreta causa\n",
        "de que proceden de su grado ? ! O Dios mio, qué alto don !\n",
        "51. CEL. __ Pues más le pedí.\n",
        "52. CAL. __ ¿ Qué, mi vieja ? ? Todas las viene las assentadas de viuir por tan bien con el baxar lo que en si no tiene orden ni\n",
        "consejo ?\n",
        "49. SEMP. __ ¡ Ha ! ! ha ! ! ha ! ¿ Esto es el fuego, diziendo que se arrima. . . Tarde fuy; pero temprano recabdé. ¡ O hermano ! ? Qué te\n",
        "contaría de sus gracias en el proceder a guardar con este modo !\n",
        "81. SEMP. __ No te marauilles, hija, que quien en muchas partes derrama\n",
        "su memoria. Yo me voy. Cumple, señor, que si salieres mañana, lleues reboçado vn paño,\n",
        "porque si della fueres por las puertas de\n",
        "tu cara. está de amor que la ver y hablar, la habla\n",
        "en nueua amanos de las puertas hazen con\n",
        "azeytes vsar su vida, como de sus mismos padres. Con todos\n",
        "tenía quehazer, con todos fablaua. Si salíamos por la calle, rogalla ? ¿ \n",
        "```\n",
        "### Observaciones\n",
        "En este caso, el modelo copia palabra a palabra varias frases directamente del texto original. Por ejemplo, \"Yo me voy. Cumple, señor, que si salieres mañana, lleues reboçado vn paño\" se encuentra así exactamente en el texto original de La Celestina. Esto sugiere overfitting.\n",
        "\n",
        "## Conclusión 100 épocas\n",
        "Vistos los tres ejemplos, cada uno con su temperatura, se puede ver que el más balanceado entre coherencia y repetitividad es el que tiene la temperatura en 0.3. Ambos son fieles al estilo del texto original. Aún así, incrementamos la cantidad de épocas para ver si el modelo podrá conseguir más coherencia, porque aunque tiene palabras y no copia frases del texto original, la sintaxis no es del todo correcta y podría tener más sentido lo que dice. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Intento 3.1: 300 épocas y 0.3 temperatura\n",
        "```\n",
        "¿Quién eres tú, que así osas turbar mi sosiego en tan alta hora? Que no hiziste seso de lo que pides ?\n",
        "76. CEL. __ No dize, hija, sino que se huelga mucho con tu amistad,\n",
        "porque eres más cierta se embueluas y con ello estés sin vn momento te partir, hasta que\n",
        "Melibea con aparejada oportunidad que en los bienes de tu hermano y el vino hazen a los hombres renegar.\n",
        "Conséjate con Séneca y verás en qué las tiene. Escucha al presente podrás ser seruida.\n",
        "11. MELIB. __ ¡ O por Dios, no se cometa tal cosa ! Pero mucho plazer\n",
        "tengo que de tan fiera hora la puerta de la virtud del río, en al es ser sano que poderlo ser, y mejor es poder ser doliente que\n",
        "ser enfermo la mia mucho de nosotros manda que con él\n",
        "mundo se oluida la mayor ni poderío en\n",
        "mi vida que Dios.\n",
        "189. PARM. __ ¿ Por qué, señor, te matas ? ¿ Por qué, señor, te contaré es no era ! y también sabes tú quánta más necessidad tienen\n",
        "los viejos que los moços, mayormente tú que vas a mi y a la desdichada de tu padre, traydo la muerte al\n",
        "couarde. ¡ O quántas mugeres\n",
        "y el\n",
        "plazer de la desdichada mia, los\n",
        "```\n",
        "\n",
        "### Observaciones\n",
        "Sigue sin ser muy coherente, pero ha tardado 16 horas en entrenarse y por lo tanto no se seguirá entrenando más de esto.\n",
        "\n",
        "## Intento 3.2: 300 épocas y 0.2 temperatura\n",
        "```\n",
        "¿Quién eres tú, que así osas turbar mi sosiego en tan alta hora? Que no ay mejor lo que deuerno a su amo haré reuessar\n",
        "el plazer comido. y tú, Elicia, alma mia, no recibas pena. Passa a mi\n",
        "casa a su propio padre.\n",
        "62. CAL. __ En todo lo dicho es no haze hábito en que ay\n",
        "clara con vn ebúrneo pensamiento todo lo que prometí, pues jamás\n",
        "al esfuerço desayudó la fortuna. Ya veo su puerta llaman y vimismos. Pues más merecido de amor, que no me destrobles la mesa ! Al\n",
        "te sabe en otra artas en el mundo yazeys mentiras, sus enemistades, con tus passadas fuerue a quebraré vn ojo que enojarte.\n",
        "84. CEL. __ No tengo ya enojo; pero porque veya que le consejaua yo lo cierto y me daua malas\n",
        "gracias. Pero de aquí del don nos en este tiempo las vía ha de yr este hecho ! No\n",
        "basta loco, no podrás escapar, si siempre no te\n",
        "acompaña quien te allegue plazeres, y la tornado de aquestos estremos, en que estoy perples no consientes como verdadera madre tuya, te digo, so\n",
        "las malediciones, que tus padres te pusieron, si me fuesse mucho lo que jamás a\n",
        "ti ni a otro pensé descobr\n",
        "```\n",
        "### Observaciones\n",
        "Es más coherente que con 0.3 de temperatura, pero toma muchas frases directo del texto, igual que en el primer intento de 100 épocas con la misma temperatura. Esto sugiere overfitting.\n",
        "\n",
        "## Intento 3.3: 300 épocas y 0.25 temperatura\n",
        "```\n",
        "¿Quién eres tú, que así osas turbar mi sosiego en tan alta hora? Que no hiziste seso allá será si lo has de salir amor, el principio\n",
        "las culpas de la riqueza. ¡ Que por de tu padre. Pues a las manos me\n",
        "has venido, donde te pueda dezir que es más cierto vso y costumbre, no\n",
        "juzgues la bondad y hermosura de Melibea por esso ser la que la madre ¿ Fize bien ?\n",
        "2. SEMP. __ ¡ Hay ! Si fiziste bien ! Allende se parescen, ladrillados por encima con\n",
        "lisonjas. /Aquél es rico que está bien con Dios. Más segura con esfuerço, no lo he oluidado ni creas que he perdido\n",
        "con los amores deste perdido\n",
        "de nuestro amo. Quiero yrme al hilo de la gente, pues a los traydores llaman\n",
        "discretos, a los fieles nescios. Si cuentas es te aprendescubre que que sujeción me relieua de culpa. No ayamos\n",
        "enojo, assentémonos a comer.\n",
        "17. ELIC. __ ¡ Assí ! ! Para esto más mal de merecer que aya vn año sería harto, no es mucha tu vida.\n",
        "63. CAL. __ ¿ Quieres dezir que soy como el moço del escuerta de liberal. Después será. Procede en tu habla y dime qué\n",
        "más passada y solicitud del principio\n",
        "```\n",
        "### Observaciones\n",
        "Es el mejor entre los tres intentos, balancea originalidad y coherencia mejor que los dos intentos anteriores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Intentos con cambios\n",
        "### EarlyStopping\n",
        "Agregamos `EarlyStopping` por lo frecuente que es la repetición en el modelo de los intentos anteriores. Siguiendo con 300 épocas, el entrenamiento ahora frena temprano. Por ejemplo, en la primera ejecución con 300 épocas frena en la época 89 con una pérdida de 0.2089.\n",
        "```\n",
        "Epoch 89/300\n",
        "58/58 ━━━━━━━━━━━━━━━━━━━━ 32s 552ms/step - loss: 0.2089\n",
        "```\n",
        "Esto indica que el modelo había dejado de mejorar significativamente y que continuar entrenando hubiera sido ineficiente y contraproducente por overfitting.\n",
        "### BatchNormalization\n",
        "Agregamos `BatchNormalization` porque estabiliza y acelera el entrenamiento al normalizar la salida de la capa `Dense`. Esto mejora la propagación del gradiente, reduce la sensibilidad a la inicialización de pesos y regulariza el modelo. También permite que la red entrene con mayor estabilidad al combinarse con activaciones como `ReLU.`\n",
        "### Dropout\n",
        "Bajamos el dropout porque el valor original (0.5) era demasiado agresivo y podía estar dificultando el aprendizaje de patrones. Al reducirlo a 0.3, se mantiene cierta regularización para prevenir el sobreajuste."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Intentos con los cambios\n",
        "## Intento 4.1: 300 épocas con early stop a las 89 épocas con pérdida de 0.2089.\n",
        "```\n",
        "¿Quién eres tú, que así osas turbar mi sosiego en tan alta hora? assí\n",
        "que con la les por el tiempo que perdí de no\n",
        "gozarlo, de no conoscerlo, después que a mí me sé conoscer. No quiero\n",
        "amar mi cara ? ¿ Qué hazes ? ¿\n",
        "Porqué soy loca ? ¿ Porqué tengo fe con este couarde ? ¿ Porqué creo sus\n",
        "mentadas ? ¡ O salud de mí ! Que se llamaua el mío, que en lor este fuego de Dios, no te destroces ni maltrates como sueles. ¿\n",
        "Qué prouecho te trae dañar mis vestiduras ?\n",
        "16. CAL. __ El te me recibirá algo de tan\n",
        "alta en su pequeño y fazer las leyes de las cosas fue dizen:\n",
        "33. SEMP. __ ¿ De qué te ríes ? ¡ De mal cancre sea comida essa boca\n",
        "desgraciada, enojosa !\n",
        "36. CEL. __ ¡ Hásteme el buen seso, viendo\n",
        "la pérdida al ojo, viendo que los atauíos hazen la muger hermosa, avnque\n",
        "no lo sepa: ¿ Hauíale yo de comenido ? ¿ A quién daré\n",
        "parte de mi gloria ? Bien me dezía la vieja que de ninguna prosperidad\n",
        "es bueno a mis passados y a mí, érades compañeros; mas, quando\n",
        "el vil está rico, no tiene pariente ni amigos ? Pues, loado Dios, bienes tienes. ¿ Y no sabes que has\n",
        "men\n",
        "```\n",
        "### Observaciones\n",
        "Esta versión del modelo genera texto más gramaticalmente coherente y estructuralmente aceptable. La fieldad al texto original sigue igual que en las versiones anteriores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Conclusión\n",
        "Al principio pensé que los problemas podrían llegar a ser con la cantidad de entrenamiento que recibe el modelo, e incrementé de 100 a 300 épocas. Aún así el modelo no lograba formar frases coherentes, y al poner el EarlyStopping me dí cuenta que la gramática mejoraba bastante, no por la cantidad de épocas pero por la incorporación del BatchNormalization y la reduccio2n del Dropout."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
